{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "#Basic\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "\n",
    "#Networks\n",
    "import networkx as ntx\n",
    "\n",
    "#Visualizing Tool\n",
    "from graph_tool.all import *\n",
    "import graph_tool.draw as gd\n",
    "import graph_tool.centrality as gc\n",
    "import cairo\n",
    "import inspect\n",
    "from graph_tool.draw.graphviz_draw import *\n",
    "from graph_tool.generation import *\n",
    "\n",
    "#Fractal Dimension\n",
    "import json\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate output file name\n",
    "def png_namegen(dtype='FB', gtype='ESpS', BAedge=None, wtype=None, eps=None, cutoff = None):\n",
    "    name = './graphs/'+gtype+'/'+('_'.join([dtype,gtype]))\n",
    "    if BAedge: name = '_'.join([name,'BA',str(BAedge)])\n",
    "    if wtype: name = '_'.join([name,'wtype',wtype])\n",
    "    if eps: name = '_'.join([name,'eps','{:.1e}'.format(eps)])\n",
    "    if cutoff: name = '_'.join([name,'cut',str(cutoff)])\n",
    "    name += '.png'\n",
    "    return name\n",
    "def hist_namegen(dtype='FB', htype='degree', BAedge=None, st=None, wtype=None, breaks=None, rang=None):\n",
    "    name = './histograms/'+htype+'/'+('_'.join([dtype,htype]))\n",
    "    if BAedge: name = '_'.join([name,'BA',str(BAedge)])\n",
    "    if st: name = '_'.join([name,'st',str(st)])\n",
    "    if wtype: name = '_'.join([name,'wtype',wtype])\n",
    "    if breaks: name = '_'.join([name,'breaks',str(breaks)])\n",
    "    if rang: name = '_'.join([name,'range','{:.1e}'.format(rang)])\n",
    "    name += '.png'\n",
    "    return name\n",
    "def adjlist_namegen(dtype='FB', gtype='ESpS', BAedge=None, wtype=None, eps=None, cutoff = None):\n",
    "    name = './AdjList/'+gtype+'/'+('_'.join([dtype,gtype]))\n",
    "    if BAedge: name = '_'.join([name,'BA',str(BAedge)])\n",
    "    if wtype: name = '_'.join([name,'wtype',wtype])\n",
    "    if eps: name = '_'.join([name,'eps','{:.1e}'.format(eps)])\n",
    "    if cutoff: name = '_'.join([name,'cut',str(cutoff)])\n",
    "    name += '.txt'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dehash and Get Adjacency List\n",
    "def dehash_and_listize(G, undir = True):\n",
    "    hasher = []\n",
    "    for e in G:\n",
    "        hasher.append(e[0])\n",
    "        hasher.append(e[1])\n",
    "    hasher = list(set(hasher))\n",
    "    print(hasher[:50])\n",
    "    dehasher = {}\n",
    "    for i, h in enumerate(hasher):\n",
    "        dehasher[h] = i \n",
    "    \n",
    "    n = len(hasher)\n",
    "    adj_G = [[] for _ in range(n)]\n",
    "    for e in G:\n",
    "        if e[0] == e[1]: continue\n",
    "        adj_G[dehasher[e[0]]].append(dehasher[e[1]])\n",
    "        if not undir:\n",
    "            adj_G[dehasher[e[1]]].append(dehasher[e[0]])\n",
    "    #print(type(G))\n",
    "    return hasher, adj_G\n",
    "\n",
    "\n",
    "#Get Datasets\n",
    "FB = np.loadtxt(fname = adjlist_namegen(dtype='FB', gtype='Raw', BAedge=None), delimiter=' ', comments='#', dtype=int)\n",
    "GR = np.loadtxt(fname = adjlist_namegen(dtype='GR', gtype='Raw', BAedge=None), delimiter=' ', comments='#', dtype=int)\n",
    "SWNwH = np.loadtxt(fname = adjlist_namegen(dtype='SWNwH', gtype='Raw', BAedge=None), delimiter=' ', comments='#', dtype=int)\n",
    "SWNwA = np.loadtxt(fname = adjlist_namegen(dtype='SWNwA', gtype='Raw', BAedge=None), delimiter=' ', comments='#', dtype=int)\n",
    "ban_1 = np.loadtxt(fname = adjlist_namegen(dtype='BAN', gtype='Raw', BAedge=1), delimiter=' ', comments='#', dtype=int)\n",
    "ban_2 = np.loadtxt(fname = adjlist_namegen(dtype='BAN', gtype='Raw', BAedge=2), delimiter=' ', comments='#', dtype=int)\n",
    "ban_3 = np.loadtxt(fname = adjlist_namegen(dtype='BAN', gtype='Raw', BAedge=3), delimiter=' ', comments='#', dtype=int)\n",
    "ban_4 = np.loadtxt(fname = adjlist_namegen(dtype='BAN', gtype='Raw', BAedge=4), delimiter=' ', comments='#', dtype=int)\n",
    "ban_5 = np.loadtxt(fname = adjlist_namegen(dtype='BAN', gtype='Raw', BAedge=5), delimiter=' ', comments='#', dtype=int)\n",
    "ban_6 = np.loadtxt(fname = adjlist_namegen(dtype='BAN', gtype='Raw', BAedge=6), delimiter=' ', comments='#', dtype=int)\n",
    "ban_7 = np.loadtxt(fname = adjlist_namegen(dtype='BAN', gtype='Raw', BAedge=7), delimiter=' ', comments='#', dtype=int)\n",
    "ban_8 = np.loadtxt(fname = adjlist_namegen(dtype='BAN', gtype='Raw', BAedge=8), delimiter=' ', comments='#', dtype=int)\n",
    "ban_9 = np.loadtxt(fname = adjlist_namegen(dtype='BAN', gtype='Raw', BAedge=9), delimiter=' ', comments='#', dtype=int)\n",
    "ban_10 = np.loadtxt(fname = adjlist_namegen(dtype='BAN', gtype='Raw', BAedge=10), delimiter=' ', comments='#', dtype=int)\n",
    "\n",
    "hasher_GR,adjlist_GR = dehash_and_listize(GR, undir = False)\n",
    "hasher_FB,adjlist_FB = dehash_and_listize(FB, undir = False)\n",
    "hasher_SWNwH,adj_swn_with_hub = dehash_and_listize(SWNwH, undir = False)\n",
    "hasher_SWNwA,adj_swn_with_add = dehash_and_listize(SWNwA, undir = False)\n",
    "hasher_ban_1,adj_ban_1 = dehash_and_listize(ban_1, undir = False)\n",
    "hasher_ban_2,adj_ban_2 = dehash_and_listize(ban_2, undir = False)\n",
    "hasher_ban_3,adj_ban_3 = dehash_and_listize(ban_3, undir = False)\n",
    "hasher_ban_4,adj_ban_4 = dehash_and_listize(ban_4, undir = False)\n",
    "hasher_ban_5,adj_ban_5 = dehash_and_listize(ban_5, undir = False)\n",
    "hasher_ban_6,adj_ban_6 = dehash_and_listize(ban_6, undir = False)\n",
    "hasher_ban_7,adj_ban_7 = dehash_and_listize(ban_7, undir = False)\n",
    "hasher_ban_8,adj_ban_8 = dehash_and_listize(ban_8, undir = False)\n",
    "hasher_ban_9,adj_ban_9 = dehash_and_listize(ban_9, undir = False)\n",
    "hasher_ban_10,adj_ban_10 = dehash_and_listize(ban_10, undir = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Dataset\n",
    "def export(adj_L, filename):\n",
    "    n = len(adj_L)\n",
    "    f = open(filename, 'w')\n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            f.write(str(i) + ' ' + str(j) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Erdos-Renyl Random Graph w/ |V|=n, prob=p\n",
    "def erdos_renyi(n = 5000, p = 0.5):\n",
    "    adj_L = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if random.random() < p:\n",
    "                adj_L[i].append(j)\n",
    "                adj_L[j].append(i)\n",
    "    return adj_L\n",
    "\n",
    "\n",
    "#Generate Modified Small-World Network w/ |V|=n+hubs, short edge:pick from 2k & prob=b, long edge = add, #hub = hubs, deg(hubs)=n*phub\n",
    "def small_world_network(n = 5000, k = 20, b = 0.5, add = 0, hubs = 0, phub = 0.1):    \n",
    "    # 2kb ~ mean deg, add : additional random edge \n",
    "    G = [[] for _ in range(n + hubs)]\n",
    "    for i in range(n):\n",
    "        for j in range(1,k+1):\n",
    "            if random.random() < b:\n",
    "                t = (i+j)%n\n",
    "                G[i].append(t)\n",
    "                G[t].append(i)\n",
    "    S = set()\n",
    "    while len(S) < add:\n",
    "        i = random.randint(0,n-1)\n",
    "        j = random.randint(i+k+1,i+n-k-1)\n",
    "        j %= n\n",
    "        if i > j:\n",
    "            i, j = j, i\n",
    "        S.add((i,j))\n",
    "    #print(S)\n",
    "    for elem in S:\n",
    "        G[elem[0]].append(elem[1])\n",
    "        G[elem[1]].append(elem[0])\n",
    "    \n",
    "    hubset = [i for i in range(n)]\n",
    "    for i in range(hubs):\n",
    "        random.shuffle(hubset)\n",
    "        for j in range(int(len(hubset) * phub)):\n",
    "            G[hubset[j]].append(n + i)\n",
    "            G[n + i].append(hubset[j])\n",
    "        hubset.append(n + i)\n",
    "    return G\n",
    "\n",
    "\n",
    "#Generate Scale-Free Network with Barabasi-Albert model w/ |V|=n, |E|=mn (new node: m edges)\n",
    "def barabasi_albert(n = 5000, m=4):\n",
    "    G = ntx.barabasi_albert_graph(n=n, m=m)\n",
    "    adj = ntx.to_dict_of_lists(G)\n",
    "    adj_L = [[] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        adj_L[i] = adj[i]\n",
    "    return adj_L\n",
    "\n",
    "\n",
    "#TEST ADJ\n",
    "adj_swn = small_world_network(20, 3, 0.7,3, 3, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Graph w/ AdjList\n",
    "def draw_network(adj_L, pos = None, output = None):\n",
    "    g = Graph(directed = False)\n",
    "    n = len(adj_L)\n",
    "    vlist = g.add_vertex(n)\n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            if i > j:\n",
    "                continue\n",
    "            e = g.add_edge(g.vertex(i), g.vertex(j))\n",
    "    if not pos:\n",
    "        pos = gd.sfdp_layout(g)\n",
    "    graph_draw(g,pos=pos,output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Efficiency w/ AdjList\n",
    "def global_efficiency(adj_L):\n",
    "    n = len(adj_L)\n",
    "    dis = [[n + 1] * n for _ in range(n)]\n",
    "    for source in range(n):\n",
    "        if source % 500 == 0:\n",
    "            print(source)\n",
    "        D = dis[source]\n",
    "        D[source] = 0\n",
    "        Q = deque([])\n",
    "        Q.append(source)\n",
    "        while len(Q) > 0:\n",
    "            f = Q.popleft()\n",
    "            for u in adj_L[f]:\n",
    "                if D[u] > D[f] + 1:\n",
    "                    D[u] = D[f] + 1\n",
    "                    Q.append(u)\n",
    "    \n",
    "    glob_eff = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            if dis[i][j] == n + 1:\n",
    "                continue\n",
    "            glob_eff += 1 / dis[i][j]\n",
    "            \n",
    "    glob_eff /= n * (n-1) // 2\n",
    "    print('global efficiency : ', glob_eff)\n",
    "    \n",
    "    return glob_eff, dis\n",
    "\n",
    "\n",
    "def local_efficiency(adj_L, use_log = True):\n",
    "    n = len(adj_L)\n",
    "    print(\"n = \", n)\n",
    "    adj_M = [[False]*n for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            adj_M[i][j] = True\n",
    "    tot_loc_eff = 0\n",
    "    var_loc_eff = 0\n",
    "    for source in range(n):\n",
    "        if source % 500 == 0:\n",
    "            print(source)\n",
    "        B = adj_L[source]\n",
    "        k = len(B)\n",
    "        if k == 0:\n",
    "            print(\"NERD\")\n",
    "            continue\n",
    "        dis = [[k + 1] * k for _ in range(k)]\n",
    "        for i in range(k):\n",
    "            dq = deque([i])\n",
    "            D = dis[i]\n",
    "            D[i] = 0\n",
    "            while len(dq) > 0:\n",
    "                f = dq.popleft()\n",
    "                for j in range(len(B)):\n",
    "                    if not adj_M[B[i]][B[j]] or D[j] <= D[i] + 1:\n",
    "                        continue\n",
    "                    D[j] = D[i] + 1\n",
    "                    dq.append(j)\n",
    "        loc_eff = 0\n",
    "        for i in range(k):\n",
    "            for j in range(i+1, k):\n",
    "                if dis[i][j] == k + 1:\n",
    "                    continue\n",
    "                loc_eff += 1 / dis[i][j]\n",
    "        if k > 1:\n",
    "            loc_eff /= k * (k-1) / 2\n",
    "        tot_loc_eff += loc_eff\n",
    "        var_loc_eff += loc_eff ** 2\n",
    "    tot_loc_eff /= n\n",
    "    var_loc_eff = var_loc_eff / n - tot_loc_eff ** 2\n",
    "    print(\"Local efficiency : \", tot_loc_eff, \"\\nVariance : \", var_loc_eff)\n",
    "    if use_log :\n",
    "        f = open(log_name, 'at')\n",
    "        f.write(\"Local efficiency : {0:.3f}\\nVariance : {1:.3f}\\n\".\n",
    "                format(tot_loc_eff, var_loc_eff))\n",
    "        f.close()\n",
    "    \n",
    "    return tot_loc_eff, var_loc_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UnionFind  ///  Trim Network w.r.t. size of Conn.Comp. (Only Strictly Greater) w/ AdjList & cutoff\n",
    "def uf_parent(x, par):\n",
    "    if par[x] == x:\n",
    "        return x\n",
    "    else:\n",
    "        par[x] = uf_parent(par[x], par)\n",
    "        return par[x]\n",
    "\n",
    "def uf_merge(x, y, par, sz):\n",
    "    x = uf_parent(x, par)\n",
    "    y = uf_parent(y, par)\n",
    "    if x != y:\n",
    "        par[y] = x\n",
    "        sz[x] += sz[y]\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def trim_network(adj_L, cutoff = 20):\n",
    "    sys.setrecursionlimit(10**6)\n",
    "    n = len(adj_L)\n",
    "    par = [i for i in range(n)]\n",
    "    sz = [1] * n\n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            uf_merge(i, j, par, sz)\n",
    "    \n",
    "    v_tr = []\n",
    "    for i in range(n):\n",
    "        if sz[uf_parent(i,par)] > cutoff:\n",
    "            v_tr.append(i)\n",
    "    \n",
    "    hasher = [-1] * n\n",
    "    for i, v in enumerate(v_tr):\n",
    "        hasher[v] = i\n",
    "    \n",
    "    ret_L = []\n",
    "    for v in v_tr:\n",
    "        x = []\n",
    "        for u in adj_L[v]:\n",
    "            x.append(hasher[u])\n",
    "        ret_L.append(x[:])\n",
    "    \n",
    "    return ret_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Minimal W Spanning Tree w.r.t. Large Deg -> Low Weight.   w/ AdjList & weight_type = 'plus'/'max'\n",
    "#Generate Spanning Graph w.r.t. Trim Edges with Small Deg.   w/ AdjList & cutoff & weight_type = 'plus'/'max'\n",
    "def elitism_spanning_tree(adj_L, weight_type='plus'):\n",
    "    n = len(adj_L)\n",
    "    deg = [len(adj_L[i]) for i in range(n)]\n",
    "    adj_sp = [[] for _ in range(n)]\n",
    "    par = [i for i in range(n)]\n",
    "    sz = [1] * n\n",
    "    \n",
    "    weighted_edges = []\n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            if i < j: continue\n",
    "            weight = 1/(deg[i] + deg[j]) if weight_type == 'plus' else 1/max(deg[i],deg[j])\n",
    "            weighted_edges.append((weight, i, j))\n",
    "    \n",
    "    weighted_edges.sort(key = lambda t : t[0])\n",
    "    print('edges : ', len(weighted_edges))\n",
    "    for w in weighted_edges:\n",
    "        if uf_merge(w[1], w[2], par, sz):\n",
    "            #print('connecting ',w[1],w[2])\n",
    "            adj_sp[w[1]].append(w[2])\n",
    "            adj_sp[w[2]].append(w[1])\n",
    "    return adj_sp\n",
    "\n",
    "def elitism_spanning_subgraph(adj_L, cutoff = 0.1, weight_type='plus'):\n",
    "    n = len(adj_L)\n",
    "    deg = [len(adj_L[i]) for i in range(n)]\n",
    "    adj_sp = [[] for _ in range(n)]\n",
    "    \n",
    "    weighted_edges = []\n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            if i < j: continue\n",
    "            weight = 1/(deg[i] + deg[j]) if weight_type == 'plus' else 1/max(deg[i],deg[j])\n",
    "            weighted_edges.append((weight, i, j))\n",
    "    \n",
    "    weighted_edges.sort(key = lambda t : t[0])\n",
    "    print('edges : ', len(weighted_edges))\n",
    "    for w in weighted_edges:\n",
    "        if w[0] > cutoff:\n",
    "            break\n",
    "        adj_sp[w[1]].append(w[2])\n",
    "        adj_sp[w[2]].append(w[1])\n",
    "    return adj_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeleton(adj_L, draw = False, draw_all = False, pos = None, output = None):\n",
    "    n = len(adj_L)\n",
    "    M = [[0] * n for _ in range(n)]\n",
    "    cnt = 0\n",
    "    E = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            if i > j: continue\n",
    "            M[i][j] = M[j][i] = cnt\n",
    "            E.append([i,j])\n",
    "            cnt += 1\n",
    "    #print(M)\n",
    "    Bcen = [0] * cnt\n",
    "    for i in range(n):\n",
    "        if i % 500 == 0:\n",
    "            print('ckpt : ', i)\n",
    "        dq = deque([i])\n",
    "        lev = [n] * n\n",
    "        bfs_ord = []\n",
    "        lev[i] = 0\n",
    "        Bcen_v = [0] * n\n",
    "        back = [[] for _ in range(n)]\n",
    "        \n",
    "        while len(dq):\n",
    "            f = dq.popleft()\n",
    "            bfs_ord.append(f)\n",
    "            for s in adj_L[f]:\n",
    "                if lev[s] < lev[f] + 1: continue\n",
    "                if lev[s] == lev[f] + 1:\n",
    "                    back[s].append(f)\n",
    "                    continue\n",
    "                back[s].append(f)\n",
    "                lev[s] = lev[f] + 1\n",
    "                dq.append(s)\n",
    "        \n",
    "        #print('source = ', i)\n",
    "        for f in bfs_ord[::-1]:\n",
    "            Bcen_v[f] += 1\n",
    "            #print('id = ', f, 'Bcen = ', Bcen_v[f], 'lev = ', lev[f], 'back = ', back[f])\n",
    "            if len(back[f]) == 0: continue\n",
    "            Bcen_w = Bcen_v[f] / len(back[f])\n",
    "            for s in back[f]:\n",
    "                #print(M[f][s], '+= ', Bcen_w)\n",
    "                Bcen[M[f][s]] += Bcen_w\n",
    "                Bcen_v[s] += Bcen_w\n",
    "    \n",
    "    skel = [(Bcen[i], i) for i in range(cnt)]\n",
    "\n",
    "    skel.sort(key = lambda t : -t[0])\n",
    "    #print(skel)\n",
    "    par = [i for i in range(n)]\n",
    "    sz = [1] * n\n",
    "    \n",
    "    adj_skel = [[] for _ in range(n)]\n",
    "    adj_nskel = [[] for _ in range(n)]\n",
    "    for w in skel:\n",
    "        a, b = E[w[1]]\n",
    "        if uf_merge(a, b, par, sz):\n",
    "            adj_skel[a].append(b)\n",
    "            adj_skel[b].append(a)\n",
    "        else:\n",
    "            adj_nskel[a].append(b)\n",
    "            adj_nskel[b].append(a)\n",
    "    \n",
    "    if not draw:\n",
    "        return adj_skel\n",
    "            \n",
    "    g = Graph(directed = False)\n",
    "    n = len(adj_L)\n",
    "    vlist = g.add_vertex(n)\n",
    "    be = g.new_edge_property(\"double\")\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in adj_skel[i]:\n",
    "            if i > j:\n",
    "                continue\n",
    "            e = g.add_edge(g.vertex(i), g.vertex(j))\n",
    "            be[e] = Bcen[M[i][j]]\n",
    "            \n",
    "    if not pos:\n",
    "        pos = gd.sfdp_layout(g)\n",
    "    if pos == 'arf':\n",
    "        pos = gd.arf_layout(g)\n",
    "    if draw_all:\n",
    "        for i in range(n):\n",
    "            for j in adj_nskel[i]:\n",
    "                if i > j: continue\n",
    "                e = g.add_edge(g.vertex(i), g.vertex(j))\n",
    "                be[e] = Bcen[M[i][j]]\n",
    "        \n",
    "    be.a /= be.a.max() / 5\n",
    "    graph_draw(g,pos=pos,edge_pen_width=be,output=output)\n",
    "    \n",
    "    return adj_skel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Graph skeleton w/ AdjList\n",
    "def draw_network_between(adj_L, pos = None, output = None):\n",
    "    g = Graph(directed = False)\n",
    "    n = len(adj_L)\n",
    "    vlist = g.add_vertex(n)\n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            if i > j:\n",
    "                continue\n",
    "            e = g.add_edge(g.vertex(i), g.vertex(j))\n",
    "    if not pos:\n",
    "        pos = gd.sfdp_layout(g)\n",
    "    \n",
    "    bv, be = gc.betweenness(g)\n",
    "    be.a /= be.a.max() / 5\n",
    "    graph_draw(g,pos=pos,vertex_color=[1,1,1,0],vertex_fill_color=bv, edge_pen_width=be,output=output)\n",
    "    \n",
    "    \n",
    "#Visualize Graph ESpT w/ AdjList\n",
    "def draw_network_elitism(adj_L, pos = None, output = None):\n",
    "    g = Graph(directed = False)\n",
    "    n = len(adj_L)\n",
    "    vlist = g.add_vertex(n)\n",
    "    ewt = g.new_edge_property(\"double\")\n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            if i > j:\n",
    "                continue\n",
    "            e = g.add_edge(g.vertex(i), g.vertex(j))\n",
    "    if not pos:\n",
    "        pos = gd.sfdp_layout(g)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            if i > j:\n",
    "                continue\n",
    "            e = g.edge(i,j)\n",
    "            ewt[e] = (g.vertex(i).out_degree() + g.vertex(j).out_degree())\n",
    "    #e = g.edges().next()\n",
    "    #bv, be = gc.betweenness(g)\n",
    "    ewt.a /= ewt.a.max() / 5\n",
    "    graph_draw(g,pos=pos,vertex_color=[1,1,1,0], edge_pen_width=ewt,output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw Degree Histogram\n",
    "def get_degree_spectra(G,breaks=None,st=0, output=None):\n",
    "    deg = {}\n",
    "    for p in G:\n",
    "        #print(p[0], p[1])\n",
    "        if deg.get(p[0]) is None:\n",
    "            deg[p[0]] = 1\n",
    "        if deg.get(p[1]) is None:\n",
    "            deg[p[1]] = 1\n",
    "        deg[p[0]] = deg[p[0]] + 1\n",
    "        deg[p[1]] = deg[p[1]] + 1\n",
    "    n = max(deg.values())\n",
    "    print(n)\n",
    "    H = [0] * (n + 1)\n",
    "    for v in deg.values():\n",
    "        H[v] = H[v] + 1\n",
    "    print(H[0])\n",
    "    X = np.arange(st,n+1)\n",
    "    if not breaks:\n",
    "        breaks = len(X)\n",
    "    _ = plt.hist(X, bins = breaks, weights=H[st:n+1])\n",
    "    if output: plt.savefig(fname=output)\n",
    "\n",
    "\n",
    "#Draw Weight Histogram\n",
    "def get_weight_spectra(adj_L,breaks = 10, rang=None, log = False, wtype='plus', output = None):\n",
    "    n = len(adj_L)\n",
    "    spec = [0] * (n+n+1)\n",
    "    X = [0] + [1/(i+1) for i in range(2*n)]\n",
    "    #X = np.arange(0, 2*n+1)\n",
    "    for i in range(n):\n",
    "        for j in adj_L[i]:\n",
    "            w = len(adj_L[i]) + len(adj_L[j]) if wtype == 'plus' else max(len(adj_L[i]), len(adj_L[j]))\n",
    "            if i<j: spec[w] += 1\n",
    "    \n",
    "    _ = plt.hist(X, bins = breaks, range = rang, log = log, weights=spec)\n",
    "    if output: plt.savefig(output)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fractal Dimension\n",
    "def json_decode(fname = './'):\n",
    "    with open(fname, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        return json_data\n",
    "\n",
    "\n",
    "path = './AdjList'\n",
    "for dirname in os.listdir(path):\n",
    "    if dirname[0] == '.':\n",
    "        continue\n",
    "    path_spec = os.path.join(path, dirname)\n",
    "    #path_export_spec = os.path.join(path_export, dirname)\n",
    "    for filename in os.listdir(path_spec):\n",
    "        if filename.split('.')[1] != 'txt':\n",
    "            continue\n",
    "        #filename_export = os.path.join(path_export_spec, filename)\n",
    "        filename_spec = os.path.join(path_spec, filename)\n",
    "        subprocess.call(['../graph-sketch-fractality-master/bin/box_cover','-type=auto','-graph='+filename_spec,'-method=sketch','-random_seed=1124747'])\n",
    "\n",
    "\n",
    "def jlog_linregress(fname = None, output=None):\n",
    "    json_data = json_decode(fname)\n",
    "    \n",
    "    size = np.array(json_data['size'])\n",
    "    rad = np.array(json_data['radius'])\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(np.log10(rad), np.log10(size))\n",
    "    \n",
    "    plt.plot(np.log10(rad), np.log10(size), 'k.', label='_nolegend_')\n",
    "    plt.plot(np.log10(rad), np.log10(rad)*slope+intercept, label='y=''{:.3f}'.format(slope)+'x+''{:.3f}'.format(intercept))\n",
    "    plt.title(json_data['run']['args'][2].split('/')[-1].split('.')[0])\n",
    "    plt.xlabel('Log(rad)')\n",
    "    plt.ylabel('Log(size)')\n",
    "    for i in range(len(rad)):\n",
    "        plt.annotate(size[i], (np.log10(rad)[i], np.log10(size)[i]))\n",
    "    plt.legend()\n",
    "    fig = plt.gcf()\n",
    "    plt.show()\n",
    "    if output:\n",
    "        path_lin = ''.join(np.delete('/'.join(np.delete(json_data['run']['args'][2].split('/'),[0,1])).split('.'),-1))\n",
    "        path_lin = './Fractal/'+path_lin+'.png'\n",
    "        #print(path_lin)\n",
    "        fig.savefig(path_lin)\n",
    "    return slope\n",
    "\n",
    "\n",
    "path = './jlog'\n",
    "for name in os.listdir(path):\n",
    "    if 'jlog' not in name: continue\n",
    "    full_name = os.path.join(path, name)\n",
    "    s = jlog_linregress(full_name, output=True)\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
